
1. 初期化
- すべての状態$s\in S$, すべての行動$a \in A(s)$に対して、行動価値関数$Q(s, a) \in \mathbb R$ と方策$\pi(s) \in A(s)$を任意に初期化する。

2. 方策評価
- 繰り返し:
    - $\Delta \leftarrow 0$
    - すべての$s\in S$に対して:
        - すべての$a \in A(s)$に対して:
            - $q \leftarrow Q(s, a)$
            - $Q(s, a) \leftarrow \sum_{s', r} p(s', r |s, a) [r + \gamma \sum_{a' \in A(s')} \pi(a'|s') Q_\pi(s', a')]$
            - $\Delta \leftarrow \max(\Delta, |v - Q(s, a)|)$
- $\Delta < \theta$ となるまで繰り返す。

3. 方策改善
- $\text{policy-stable} \leftarrow \text{true}$
- すべての$s \in S$に対して
    - $\text{old-action} \leftarrow \pi(s)$
    - $\pi(s) \leftarrow \text{argmax}_a Q(s, a)$           
    - $\text{policy-stable} \leftarrow \text{policy-stable} \land \text{old-action} = \pi(s)$
- $\text{policy-stable}$ならば停止。総出なければ2へ飛ぶ